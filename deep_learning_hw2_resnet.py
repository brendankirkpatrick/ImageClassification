# -*- coding: utf-8 -*-
"""Deep Learning HW2 ResNet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WMXcjXnOI2dzHomzMrEXaVCaQgljGoLL
"""

#import packages
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Dense, Input, Activation, BatchNormalization, ZeroPadding2D, MaxPool2D
from tensorflow.keras.layers import Conv2D, Dropout, Flatten, Add, AveragePooling2D
from tensorflow.keras.datasets import mnist
from tensorflow.keras import backend as k
import sklearn
import sklearn.model_selection
import matplotlib.pyplot as plt

(x_train, y_train), (x_test, y_test) = mnist.load_data()

## checking format of the data to see if it is channels first or last
print(k.image_data_format() == 'channels_first')
#since it is not channels first
img_rows, img_cols=28, 28

x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)
x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)
inpx = (img_rows, img_cols, 1)

#convert to float datatype to divide by 255
x_train = x_train.astype('float32')
x_test = x_test.astype('float32')
x_train /= 255
x_test /= 255

# one-hot encoding for all the target labels
y_train = tf.keras.utils.to_categorical(y_train)
y_test = tf.keras.utils.to_categorical(y_test)

### building identity and convolutional blocks

def identity_block(x, filter):
    # copy tensor to variable called x_skip
    # initial tensor is passed to x_skip to bypass the following layers
    x_skip = x
    # Layer 1
    x = Conv2D(filter, (3,3), padding = 'same')(x)
    x = BatchNormalization(axis=3)(x)
    x = Activation('relu')(x)
    # Layer 2
    x = Conv2D(filter, (3,3), padding = 'same')(x)
    x = BatchNormalization(axis=3)(x)
    # Add Residual before the activation function of layer 2
    x = Add()([x, x_skip])
    x = Activation('relu')(x)
    return x

def convolutional_block(x, filter):
    # copy tensor to variable called x_skip
    # initial tensor is passed to x_skip to bypass the following layers
    x_skip = x
    # Layer 1
    x = Conv2D(filter, (3,3), padding = 'same', strides = (2,2))(x)
    x = BatchNormalization(axis=3)(x)
    x = Activation('relu')(x)
    # Layer 2
    x = Conv2D(filter, (3,3), padding = 'same')(x)
    x = BatchNormalization(axis=3)(x)
    # Processing Residual with conv(1,1)
    x_skip = Conv2D(filter, (1,1), strides = (2,2))(x_skip)
    # Add Residue
    x = Add()([x, x_skip])
    x = Activation('relu')(x)
    return x

### building ResNet18 model

def ResNet18(shape = inpx, classes = 10):
    #Implementation of the popular ResNet50 the following architecture:
    #CONV2D -> BATCHNORM -> RELU -> MAXPOOL -> IDBLOCK*3 -> CONVBLOCK -> IDBLOCK*3 -> CONVBLOCK -> IDBLOCK*5
    #-> CONVBLOCK -> IDBLOCK*2 -> AVGPOOL -> Dense/RELU -> Dense/SOFTMAX

    # Step 1 (Setup Input Layer)
    x_input = Input(shape)
    x = ZeroPadding2D((3, 3))(x_input)

    # Step 2 (Initial Conv layer along with maxPool)
    x = Conv2D(64, kernel_size=7, strides=2, padding='same')(x)
    x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = MaxPool2D(pool_size=3, strides=2, padding='same')(x)

    # Define size of sub-blocks and initial filter size
    #[2,2,2,2] for Resnet18
    block_layers = [2,2,2,2]
    filter_size = 64

    # Step 3 Add the Resnet Blocks
    for i in range(4):
        if i == 0:
            # For sub-block 1 Residual/Convolutional block not needed
            for j in range(block_layers[i]):
                x = identity_block(x, filter_size)
        else:
            # One Residual/Convolutional Block followed by Identity blocks
            # The filter size will go on increasing by a factor of 2
            filter_size = filter_size*2
            x = convolutional_block(x, filter_size)
            for j in range(block_layers[i] - 1):
                x = identity_block(x, filter_size)

    # Step 4 End Dense Network
    x = AveragePooling2D((2,2), padding = 'same')(x)
    x = Flatten()(x)
    x = Dense(512, activation = 'relu')(x)
    x = Dense(classes, activation = 'softmax')(x)
    model = Model(inputs = x_input, outputs = x, name = "ResNet34")
    return model

model = ResNet18()

## Compile the Model
LR = 0.0001

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate= LR),
              loss=tf.keras.losses.categorical_crossentropy,
              metrics=['accuracy','Precision','Recall'])
# add precision and recall metrics to compute F1

## fitting model

from keras.callbacks import ModelCheckpoint, EarlyStopping

# path chosen to save the model
filepath = "best_model.ResNet_0.0001"

# initialize the ModelCheckpoint callback and early stopping
checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')
es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)

# fit model
history = model.fit(x_train, y_train, epochs=30, batch_size=100, validation_split= 0.2, callbacks=[es, checkpoint])

## graphing
def get_plot(history):
    # compute F1 score from precision and recall
    #convert lists to arrays for multiplication
    prec_array = np.array(history.history['precision'])
    rec_array = np.array(history.history['recall'])
    val_prec_array = np.array(history.history['val_precision'])
    val_rec_array = np.array(history.history['val_recall'])

    training_F1 = 2*prec_array*rec_array / (prec_array + rec_array)
    validation_F1 = 2*val_prec_array*val_rec_array / (val_prec_array + val_rec_array)

    # plot the loss and performance of the training and validation from the model.fit
    training_loss = history.history['loss']
    validation_loss = history.history['val_loss']
    training_acc = history.history['accuracy']
    validation_acc = history.history['val_accuracy']

    # Create subplots for loss
    plt.figure(figsize=(12, 8))

    # Creating training and validation loss plot
    plt.subplot(3, 1, 1)
    plt.plot(training_loss, label='Training Loss')
    plt.plot(validation_loss, label='Validation Loss')
    plt.title('Training and Validation Loss: ResNet, LR=' + str(LR))
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    # creating training and validation accuracy plot
    plt.subplot(3, 1, 2)
    plt.plot(training_loss, label='Training')
    plt.plot(validation_loss, label='Validation')
    plt.title('Training and Validation Accuracy: ResNet, LR=' + str(LR))
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()

    # creating training and validation F1 plot
    plt.subplot(3, 1, 3)
    plt.plot(training_F1, label='Training')
    plt.plot(validation_F1, label='Validation')
    plt.title('Training and Validation F1 Score: ResNet, LR=' + str(LR))
    plt.xlabel('Epoch')
    plt.ylabel('F1 Score')
    plt.legend()

    ### AUC plot of test dataset

    # Adjust layout and show the plot
    plt.tight_layout()
    plt.show()

get_plot(history)

def test_model(file_path):
    #load the trained model and the test dataset to predict
    from keras.models import load_model

    # load the saved model
    saved_model = load_model(file_path)
    pred = saved_model.predict(x_test)
    pred_classes = np.argmax(pred, axis= 1)
    y_test_classes = np.argmax(y_test, axis= 1)

    #pred = pred.flatten()

    #Report accuracy
    acc_pred = tf.keras.metrics.Accuracy()
    acc_pred.update_state(y_test_classes, pred_classes)
    Acc = acc_pred.result().numpy()
    print('Accuracy:',Acc)

    #Report F1
    from sklearn.metrics import f1_score
    F1 = f1_score(y_test_classes, pred_classes, average='weighted')
    print("F1 Score:",F1)

# plot the first and second layer feature maps for CNN based models
def feauture_visualization(layer_name, model, input_data):
    # grab the first two conv layers when entered in layer name
    intermediate_layer_model = Model(inputs=model.input, outputs=model.get_layer(layer_name).output)

    # predict on x_test to get feature maps
    intermediate_output = intermediate_layer_model.predict(input_data)

    # Number of feature maps
    num_feature_maps = intermediate_output.shape[-1]

    # Calculate the number of rows and columns for subplots
    num_rows = int(np.ceil(num_feature_maps / 8))
    num_cols = min(num_feature_maps, 8)

    # Create a figure to display the feature maps
    plt.figure(figsize=(12, 2 * num_rows))
    for i in range(num_feature_maps):
        plt.subplot(num_rows, num_cols, i + 1)
        plt.imshow(intermediate_output[0, :, :, i], cmap='viridis')
        plt.axis('off')
    plt.show()

def AUC_ROC(file_path):
    #load the trained model and the test dataset to predict
    from keras.models import load_model

    # load the saved model and make predictions
    saved_model = load_model(file_path)
    pred = saved_model.predict(x_test)
    pred_classes = np.argmax(pred, axis= 1)
    y_test_classes = np.argmax(y_test, axis= 1)

    from scipy import interp
    from sklearn.metrics import roc_curve, auc

    fpr = dict()
    tpr = dict()
    roc_auc = dict()
    classes = 10

    # get the FPs and TPs from y test and predicted values and calc auc from the FP and TP rates
    for i in range(classes):
        fpr[i], tpr[i], _ = roc_curve(y_test[:, i], pred[:, i])
        roc_auc[i] = auc(fpr[i], tpr[i])

    # compute micro-avg ROC curve and ROC area
    fpr["micro"], tpr["micro"], _ = roc_curve(y_test.ravel(), pred.ravel())
    roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

    # Plot roc-auc curve for all classes.
    from itertools import cycle

    plt.figure()
    plt.plot(fpr["micro"], tpr["micro"],
            label='micro-average ROC curve (AUC = {0:0.2f})'
                  ''.format(roc_auc["micro"]),
            color='deeppink', linestyle=':', linewidth=4)

    colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])
    for i, color in zip(range(classes), colors):
        plt.plot(fpr[i], tpr[i], color=color, lw=2,
                label='ROC curve of class {0} (AUC = {1:0.2f})'
                ''.format(i, roc_auc[i]))

    plt.plot([0, 1], [0, 1], 'k--', lw=2)
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC One-vs-Rest Multi-Class\nResNet: LR(0.001)')
    plt.legend(loc="lower right", fontsize="7")
    plt.show()

from google.colab import drive
drive.mount('/content/drive')

filename = "best_model.ResNet_0.0001"
drive_folder_path = "/content/drive/My Drive/Homework 2/"
full_drive_path = drive_folder_path + filename

#load best model to be saved
from keras.models import load_model
saved_model = load_model(filename)

# Save the model to Google Drive
saved_model.save(full_drive_path)

filepath = "/content/drive/My Drive/Homework 2/best_model.ResNet_0.0001"
test_model(filepath)

AUC_ROC(filepath)

## plot the feature maps
#load trained model
from keras.models import load_model
trained_model = load_model(filepath)

# Plot feature maps for the first convolutional layer
feauture_visualization('conv2d_40', trained_model, x_test[0:1])

# Plot feature maps for the second convolutional layer
feauture_visualization('conv2d_41', trained_model, x_test[0:1])

## save model weights
trained_model.save_weights("/content/drive/My Drive/Homework 2/best_model.ResNet_weights.h5")

filepath = "/content/drive/My Drive/Homework 2/best_model.ResNet_0.001"
#load trained model
from keras.models import load_model
trained_model = load_model(filepath)
trained_model.summary()